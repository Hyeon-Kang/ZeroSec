{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Json 등 형식에 맞는 파일 불러오기\\n2. article text를 추출\\n3. stop word 적용\\n4. text rank 핵심 문장으로 요약\\n5. //konply에서 명사만 추출 ex) okt.nouns(string)\\n6. 키워드 추출 (ex: TF-IDF)\\n7. 아니면 핵심 문장간 유사도 비교\\n8. 합당한 문장에서 지명 대조\\n9. 추출한 지명 따로 저장\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Json 등 형식에 맞는 파일 불러오기\n",
    "2. article text를 추출\n",
    "3. stop word 적용\n",
    "4. text rank 핵심 문장으로 요약\n",
    "5. //konply에서 명사만 추출 ex) okt.nouns(string)\n",
    "6. 키워드 추출 (ex: TF-IDF)\n",
    "7. 아니면 핵심 문장간 유사도 비교\n",
    "8. 합당한 문장에서 지명 대조\n",
    "9. 추출한 지명 따로 저장\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 개수 : (1526, 4)\n",
      "1526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. csv file 불러오기\n",
    "import pandas as pd\n",
    "csv_data = pd.read_csv('201910article.csv', delimiter=',', encoding='UTF-8')\n",
    "\n",
    "# 행렬 개수 확인\n",
    "print(\"데이터 개수 :\",csv_data.shape)\n",
    "print(csv_data.shape[0])\n",
    "print()\n",
    "\n",
    "#csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    옆집이 성매매 오피스텔 중 형량은 낮고 돈은 많이 번다“5년간 오피 운영하다 적발 ...\n",
      "1               조국 법무부 장관이 1일 서울 여의도 국회 본회의장에서 열린 제...\n",
      "2    “학교에 검찰 압수수색 나오기 사흘 전쯤 정경심 교수가 나에게 전화해 ‘혹시 압수수...\n",
      "3               윤석열 검찰총장 © 1 황기선 기자          서울뉴스1 ...\n",
      "4               제17호 태풍 타파로 강풍이 몰아치는 강원도 안인진 해변 자료사...\n",
      "5               【서울뉴시스】 전진환 기자  윤석열 검찰총장이 지난달 30일 오...\n",
      "6    황교안  자유한국당 대표 투쟁은 문희상 의장 민주당 또 그 2중대 3중대의 불법적 ...\n",
      "7               사진순천경찰서          전남 순천 한 종합병원에서 몰래카...\n",
      "8    경향신문 ㆍ“대통령에 대한 압박이자 인사권에 정면 도전” 비판ㆍ“이 정도 됐으면 물...\n",
      "9               서울 서초구 대검찰청 2019101뉴스1 © 1 구윤성 기자  ...\n",
      "Name: article, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2. article 컬럼만 추출(원본), sentence(특수문자, stop word 적용)\n",
    "article_list = csv_data.article\n",
    "print(article_list[:10])\n",
    "#article_list[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특수문자 제거 sentence[1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3_1. 특수문자 제거\n",
    "\n",
    "# 소문자 변환\n",
    "# re 라이브러리로 특수문자 제거\n",
    "\n",
    "# 읽어온 데이터 확인 (json_body)\n",
    "\n",
    "sentence = []\n",
    "\n",
    "# 특수문자 제거 라이브러리\n",
    "import re\n",
    "for a in range(len(article_list)):\n",
    "    sentence.append(article_list[a].strip())\n",
    "    \n",
    "for i in range(len(sentence)):\n",
    "    sentence[i] = sentence[i].lower() # lower() : 모든 문자를 소문자로 변환\n",
    "    sentence[i] = re.sub(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\", ' ', sentence[i]) # 특수문자 제거\n",
    "    sentence[i] = re.sub(\"[.;:!\\'?,\\\"()|[\\]]\", ' ', sentence[i]) # 특수문자 제거\n",
    "\n",
    "print(\"특수문자 제거 sentence[1]\") # 모든 article을 sentence에 저장 후 특수문자 제거 및 (영어)소문자화 \n",
    "#print(sentence[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3_2. 외부 Stop Word 리스트 불러오기 (stop word ranking 기반) \n",
    "# 불용어 처리 - stop_word 리스트 불러오기 & 전처리\n",
    "stop_words = []\n",
    "f = open(\"C:\\Python\\source\\stop_word.txt\", 'r', encoding='UTF-8')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    #print(line) #정상 출력 확인\n",
    "    stop_words.append(line.rstrip('\\n')) # 개행문자 제거 후 추가\n",
    "f.close()\n",
    "\n",
    "# 불러온 stop word 리스트 출력\n",
    "#print(stop_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop word 필터링 결과 (result)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3_3. 불러온 Stop Word 적용  \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "result = [] \n",
    "for w in sentence: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "\n",
    "# 위의 4줄은 아래의 한 줄로 대체 가능\n",
    "# result=[word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "# 불용어 처리 결과 출력\n",
    "print(\"stop word 필터링 결과 (result)\")\n",
    "#print(result[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1526, 124781)\n"
     ]
    }
   ],
   "source": [
    "# 6 TF-IDF 모델 생성 및 그래프 생성\n",
    "#!pip install sklearn\n",
    "#!pip install scikit-learn\n",
    "#!pip install scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X = sentence\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아프리카 포함 인덱스 [163, 166, 231, 275, 295, 386, 401, 536, 567, 868, 887, 917, 926, 946, 955, 1035, 1251, 1254, 1267, 1367, 1388, 1516]\n",
      "돼지 포함 인덱스 [78, 163, 166, 189, 231, 275, 295, 386, 536, 666, 845, 868, 887, 917, 926, 942, 955, 1251, 1267, 1367, 1368, 1388, 1508, 1516]\n",
      "야생 포함 인덱스 [62, 217, 231, 256, 295, 410, 431, 481, 536, 887, 918, 950, 955, 983, 1035, 1267, 1283, 1388, 1516]\n",
      "방역 포함 인덱스 [25, 133, 163, 166, 231, 295, 384, 536, 557, 1251, 1267, 1368, 1388, 1516]\n",
      "멧돼지 포함 인덱스 [163, 231, 295, 536, 887, 917, 926, 955, 1267, 1367, 1388, 1516]\n",
      "아프리카 + 돼지 인덱스 :  [386, 163, 868, 1251, 166, 295, 231, 1388, 1516, 275, 1267, 917, 887, 536, 1367, 955, 926]\n",
      "아프리카 + 돼지 + 야생 인덱스 :  [295, 231, 1388, 1516, 1267, 887, 536, 955]\n"
     ]
    }
   ],
   "source": [
    "# 특정 키워드에서 중복 요소 찾기\n",
    "keyw = ['아프리카', '돼지', '야생', '방역', '멧돼지', '발생']\n",
    "w_list0 = []\n",
    "w_list1 = []\n",
    "w_list2 = []\n",
    "w_list3 = []\n",
    "w_list4 = []\n",
    "w_list5 = []\n",
    "w_list6 = []\n",
    "w_list7 = []\n",
    "w_list8 = []\n",
    "\n",
    "# 아프리카\n",
    "for i in range(len(sentence)):\n",
    "    if( sentence[i].find(keyw[0]) >= 0 ):\n",
    "        w_list0.append(i)\n",
    "# 돼지\n",
    "for i in range(len(sentence)):\n",
    "    if( sentence[i].find(keyw[1]) >= 0 ):\n",
    "        w_list1.append(i)\n",
    "# 야생\n",
    "for i in range(len(sentence)):\n",
    "    if( sentence[i].find(keyw[2]) >= 0 ):\n",
    "        w_list2.append(i)\n",
    "# 방역\n",
    "for i in range(len(sentence)):\n",
    "    if( sentence[i].find(keyw[3]) >= 0 ):\n",
    "        w_list3.append(i)\n",
    "# 멧돼지  \n",
    "for i in range(len(sentence)):\n",
    "    if( sentence[i].find(keyw[4]) >= 0 ):\n",
    "        w_list4.append(i)\n",
    "# 발생\n",
    "for i in range(len(sentence)):\n",
    "    if( sentence[i].find(keyw[5]) >= 0 ):\n",
    "        w_list5.append(i)\n",
    "\n",
    "print(\"아프리카 포함 인덱스\",w_list0)\n",
    "print(\"돼지 포함 인덱스\",w_list1)\n",
    "print(\"야생 포함 인덱스\",w_list2)\n",
    "print(\"방역 포함 인덱스\",w_list3)\n",
    "print(\"멧돼지 포함 인덱스\",w_list4)\n",
    "\n",
    "# 살처분, 확진, 보상, 신고, 야생\n",
    "# 키워드 추출 리스트에서 중첩 인덱스 찾기\n",
    "\n",
    "# 아프리카 + 돼지\n",
    "u_list = list(set(w_list0).intersection(w_list1))\n",
    "print(\"아프리카 + 돼지 인덱스 : \", u_list)\n",
    "\n",
    "# (아프리카 + 돼지) + 야생\n",
    "u_list = list(set(u_list).intersection(w_list2))\n",
    "print(\"아프리카 + 돼지 + 야생 인덱스 : \", u_list)\n",
    "\n",
    "# (돼지 + 방역 + 야생) + \n",
    "\n",
    "#비교대상 1251, 295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "포천 뉴스1 이상 휴 기자 일 경기 포천시 관인면 돼지 농장 아프리카 돼지 열병 의심 신고 접수 경기북부 축산업 비상 포천 지리 강원도 양성 판정 날 경우 의 경기 이남 지역 남 하와 수 때문 이다 포천 의심 신고 접수 농장 돼지 마리 대형 농장 이날 오전 시 돼지 마리 폐사 방역 당국 의심 축 신고 접수 포천 철원 돼지 농장 긴급 이동 제한 발효 전망 해당 농장 반경 돼지 농장 이내 농장 여 마리 돼지 사육 중 확진 판정 날 경우 해당 농장 반경 돼지 예방 살 처분 포천시 정부 통상 방역 시스템 강도 방역 체계 유지 의심 신고 접수 동요 분위기 동안 포천 관내 초소 곳 설치 방역 활동 육군 군단 군단 등 개 부대 여명 군 장병 방역 초소 시와 임무 수행 중 포천시 돼지 농장 곳 마리 사육 경기북부 최대 축산 도시 포천 방역 망이 경우 전면 강도 예방 살 처분 정책 정부 방역 대책 경기북부 돼지 매장 때문 정부 강화군 경기 김포시 파주시 돼지 살 처분 지금 파주 연천 김포 강화 등 비무장지대 민간인 통제 선 해상 북방한계선 지자체 발생 일 연천군 측 남방 한계선 전방 지점 야생 멧돼지 사체 발견 감염 상태 때문 북한 멧돼지 국내 바이러스 유입 원 지적 제기 정부 지난해 김포 등 최 전방 지대 해안 철책 철거 파주 등 여개 초소 양 돈 인 5월 북한 발생 바이러스 멧돼지 철책 사이 유입 것 의구심 제기 정부 감염 경로 파악 상태 다 포천 북한 접경 지역 의심 신고 접수 관인면 돼지 농장 비교 북한 양성 판정 날 경우 포천 전역 위험 지대 경기 남부 지역 남하 강원도 확산 위 섬 다 최 접경 지역 곳 지난달 일 양주시 의심 신고 접수 바 음성 판정 바 있다 \n",
      "\n",
      "앵커 야생 멧돼지 아프리카 돼지 열병 게 확인 그동안 멧돼지 신경 당국 비판 멧돼지 열병 동유럽 체코 사례 참고 필요 말 김 관 진 기자 입 니다 기자 지난 5월 북한 아프리카 돼지 열병 발병 환경부 접경 지 돼지 농가 주변 울타리 설치 등 차단 방역 적극 정작 국내 확진 사례 후 환경부 멧돼지 소극 대응 집 돼지 잡기 집중 동안 야생 멧돼지 사체 아프리카 돼지 열병 바이러스 검출 확인 전문가 내부 바이러스 기합 니다 파리 고양이 진드기 등 감염 멧돼지 사체 배설물 접촉 때 바이러스 수 우 희종 교수 서울대 수의학 북 측 남 측 경계선 사이 에 남 발생 북 발생 말 건 검역 본부 최근 체코 아프리카 돼지 열병 방역 사례 주목 것 체 코 병 유입 건 년 이후 2년 간 건 발병 올해 건의 확진 때문 최근 3년 간 아프리카 돼지 열병 발병 개국 가운데 체코 기간 바이러스 박멸 성공 국가 초기 집중 야생 멧돼지 포획 사살 대책 주효 평가 입 니다 경찰 군대 동원 멧돼지 개체 수 관리 사냥 보상금 체코 야산 야생 멧돼지 사살 수렵 인 활동 정현 규 양 돈 수의사 협회장 체코 멧돼지 일정 지역 절멸 시키 작업 작업 대책 지금 노력 수 포로 관리 멧돼지 바이러스 만연 경우 토착화 우려 만큼 대책 필요 영상 취재 서진 호 영상 편집 황 지영 화면 출처 유튜브 자료 제공 더불어민주당 김현 의원 실 김관진 기자 \n",
      "\n",
      "일 멧돼지 사살 모습 부산 경찰청 제공 판매 금지 전국 종합 연합뉴스 손 형주 기자 멧돼지 사체 아프리카 돼지 열병 바이러스 검출 데 도심 곳곳 멧돼지 시민 불안 일 오전 시 분 부산 동래구 주택가 멧돼지 마리 신고 접수 경찰 신고 접수 이후 시간 아파트 주차장 곳 빈집 등지 발견 멧돼지 실탄 발 발사 마리 사살 멧돼지 주차장 유리 거울 초등학교 인근 수차례 배회 뒤 어미 멧돼지 잡지 학생 안전 경찰 경계 활동 등 시민 불안 일 새벽 부산 금정구 서동 주택가 대형 멧돼지 유해 조수 기동 포획 단 사살 일 울산 울주군 국도 멧돼지 마리 차 치 떼죽음 날 오전 시 분 청주 도심 모충동 주택가 멧돼지 마리 신고 경찰 출동 멧돼지 마리에 실탄 발 사살 사살 과정 멧돼지 공격 김 모 경위 다리 상처 올해 부산 멧돼지 포획 장소 아파트 주차장 대학교 기숙사 도심 공원 등 때 장소 멧돼지 도심 출몰 있다 일 환경부 올해 1월 이달 일 포획 멧돼지 마리에 멧돼지 사살 정연주 제작 일 러스트 최근 아프리카 돼지 열병 예방 방역 차원 당국 대대적 멧돼지 포획 포획 수 탓 년 마리 년 마리 포획 숫자 상태 국립생물자원관 지난해 발표 야생동물 실태 조사 보고서 멧돼지 연도 서식 밀도 5년 간 증가세 년 헥타르 당 멧돼지 마리 서식 것 년 당 마리 서식 것 멧돼지 가을 겨울 도심 출몰 짝 기간 활동 서식 밀도 증가 먹이 부족 가을 겨울 서식지 도심 때문 도심 출몰 시민 불안 대책 제자리걸음 이다 전문가 포획 위주 대책 개체 수 감소 수 연구 병행 조언 멧돼지 발견 때 다 멧돼지 놀라 공격 모습 공격 수 때문 부산 소방 재난 본부 관계자 일정 거리 멧돼지 경우 주변 나무 바위 등 몸 신속 대피 뒤 소방 당국 신고 당부 \n",
      "\n",
      "앵커 멧돼지 소식 오늘 일 이제 바다 번 킬로미터 정도 헤엄 먹이 것 배 승주 기자 입 니다 기자 동해 앞바다 덩치 생물체 파도 어선 접근 반대쪽 길 미터 무게 킬로그램 야생 멧돼지 입 니다 최순 철근 성호 선장 배 고리 경북 영덕군 강구면 인근 앞바다 멧돼지 신고 해경 접수 건 어제 일 오후 시 쯤 해상 엽 사가 총 포획 멧돼지 미터 바다 한가운데서 처음 목격 포획 전 바다 시간 확인 것 시간 가량 입 니다 김영찬 최초 발견자 물개 인가 멧돼지 소름 멧돼지 통나무 몸매 덕분 물 바다 번 킬로미터 수 먹이 부족 영역 다툼 바다 이동 합 니다 방역 당국 멧돼지 사체 수거 아프리카 돼지 열병 등 감염 여부 확인 중 \n",
      "\n",
      "칠흑 어둠 호흡 뒤 개 시작 불 꺼 돼지 돼지 에 추적 사냥개 곳 표시 수색 시작 지 분 만 소식 산줄기 가을바람 탓 식은땀 때문 기운 온몸 멧돼지 수 긴장감 기대감 걸음 멧돼지 대신 것 오소리 굴 아쉬움 수색 계속 전병호 엽 사와 야생 멧돼지 피해 윤정식 씨 수색 활동 산소 밭 아수라장 아프리카 돼지 열병 전국 위기감 고조 일 야생 멧돼지 사냥 현장 동행 경기도 의정부 이날 수색 홍복 산 수락산 동 등지 시간 동안 차례 야생 멧돼지 수색 장비 점검 전병호 엽 사 수색 년 경력 엽 사 전병호 60 씨 일 야생 멧돼지 피해 시청 신고 윤정식 63 교수 씨 동행 평 정도 밭 고구마 옥수수 고추 멧돼지 때문 농사 포기 펜스 삼중 설치 이건 말 윤 씨 멧돼지 산소 밭 피해 막심 호소 수색 도중 발견 야생 멧돼지 배설물 야생 멧돼지 땅 흔적 육감 직감 야생 멧돼지 수색 마리 사냥개 협동 작전 이날 수색 씨 메리 수박 불 마리 사냥개 추적 조인 메리 냄새 멧돼지 발견 나머지 전투 조가 합류 멧돼지 공격 추적 조에 메리 외 개 일 멧돼지 공격 수색 참여 엽 사냥개 부착 위치 확인 추적 조가 거리 밖 발견 신호 이후 엽 사냥개 멧돼지 동안 총 멧돼지 제압 야생 멧돼지 추적 사냥개 부착 위치 확인 씨 메리 만큼 추적 사냥개 멧돼지 일 꿈 수 설명 사냥 총 산 거리 멧돼지 저격 명중 생각 눈앞 풍경 안전 안전 야생 멧돼지 사냥 것 씨 안전 안전 안전 총기 일 만큼 안전 사냥 기본 우선 과제 강조 때문 사냥개 수색 것 설명 사냥개 엽 사 투입 경우 멧돼지 움직임 예측 상황 총알 멧돼지 자극 우려 것 야생 멧돼지 추적 전병호 엽 사와 사냥개 돼지 소리 쪽 총구 총구 사람 돼지 사람 거지 씨 사람 총 귀 적도 멧돼지 사냥 온도 활동 등산객 낮 밤 때문 개 도움 필수 치 앞 야간 산행 멧돼지 것 어둠 산속 멧돼지 일 이정표 가로등 산속 것 눈 이동하 것 개 쪽 이동 때문 길 산비탈 수풀 속 짙은 어둠 뿐 산속 야생 멧돼지 수색 수색 시작 지 시간 분 쯤 때 개 중 산비탈 발 온몸 흙 사람 멧돼지 내일 온몸 상처 것 거 예 취재 후 머릿속 속옷 물론 주머니 모래 지푸라기 문자 만신창이 멧돼지 의지 개 숨 사람 말 수도 번 수색 마무리 뒤 멧돼지 신고 접수 수락산 동 막 골 이동 휴대폰 손전등 하나에 의지 등산로 산길 일 적응 멧돼지 번 수색 산 지 분 만 하산 도로변 로드킬 멧돼지 수습 연락 때문 시간 동안 진행 수색 멧돼지 한가운데 로드킬 야생 멧돼지 사체 멧돼지 가까이 일 처음 정도 크기 개체 봄 것 추정 멧돼지 눈앞 보니 아쉬움 5월 주 회 마리 날 날 반반 전병호 엽 사 아쉬움 다음날 생업 개인택시 영업 수색 이쯤 아프리카 돼지 열병 수 일 기준 아프리카 돼지 열병 바이러스 감염 멧돼지 사체 마리 이중 민통선 발견 개체 수 마리 아프리카 돼지 열병 감염 멧돼지 민통선 남쪽 경우 바이러스 확산 수 우려 유다 환경부 바이러스 감염 멧돼지 주변 전기 펜스 높이 울타리 추가 설치 민 관구 합동 포획 팀 차 작전 마리 일 일 간의 차 작전 준비 중 전국 개 포획 틀 설치 전병호 엽 사가 사냥개 야생 멧돼지 멧돼지 총력 일각 회의 시각 것 사실 멧돼지 신체 능력 울타리 의미 저지 도구 지적 멧돼지 이동 량 합동 포획 팀 활동 반경 분석 지자체 남은 예산 부족 포획 틀 구입 방역 소요 인건비 충당 우려 엽 사 보상금 마리 당 원 지급 방안 검토 단계 수준 환경부 아프리카 돼지 열병 총괄 대응 팀 관계자 예산 보상금 등 제기 문제 파악 해결 관계 부처 협의 최선 노력 답 멧돼지 사냥 씨 만일 멧돼지 포획 성공 수 보상금 현재 상황 씨 멧돼지 공격 사냥개 병원비 원 농민 감사 인사 때 보람 일 계속 봉사 정신 절대 수 일 설명 멧돼지 미온 대응 속 아프리카 돼지 열병 확산 속도 개인 호의 봉사 정신 수 것 봉사자 선제 지원 멧돼지 확산 방지 가용 자원 총동원 이번 사태 마침표 시간 김성현 기자 저작권 자 무단 전재 배포 금지 \n",
      "\n",
      "앵커 요즘 멧돼지 도심 출몰 소식 자주 어젯 일밤 울산 마리 승용차 치 사고 차량 사람 혜지 기자 입 니다 기자 울산 도 롯 가입 니다 어미 멧돼지 마리 새끼 멧돼지 마리 한 줄 어젯밤 시 분 쯤 도로 이동 멧돼지 리가 승용차 치 국도 멧돼지 차량 현장 즉사 것 추정 차량 멧돼지 뒤 중앙 분리 대 앞 범퍼 사람 경찰 음주 측정 결과 30대 운전자 음주 상태 운전자 도로 이동 멧돼지 무리 발견 친 것 추정 박 노 양 울주 경찰서 온양 파출소 내리막길 전방 멧돼지 무리 추돌 식 차 밑 비도 관할 지자체 멧돼지 의 아프리카 돼지 열병 감염 여부 조사 원준 야생 생물 관리 협회 울산 지부장 울산 울주 건 동구 건 이 아프리카 돼지 열병 것 판정 앞으로 주민 신고 시일 검사 관리 산 지난달 멧돼지 순찰차 충돌 등 최근 달 동안 건의 멧돼지 출몰 소동 영상 취재 장 진국 박환 흠 \n",
      "\n",
      "번 아프리카 돼지 열병 확진 농가 발생 일 경기 연천군 신서면 양 돈 농가 방역 당국 관계자 살 처분 매몰 대형 플라스틱 통 운반 차량 방역 작업 이번 확진 일 김포 통진읍 확진 농가 이후 이다 연천 지난달 일 번 확진 농가 이후 추가 발생 이번 발병 방역 당국 긴장 유 승관 기자 연천 뉴스1 이상 휴 기자 우리 농장 울타리 밑바닥 부분 멧돼지 흔적 군데 발견 멧돼지 농장 안 침입 울타리 접촉 사육 돼지 아프리카 돼지 열병 것 바이러스 유입 원 야생 멧돼지 일 것 의심 검역 관 멧돼지 중심 역학 관계 조사 호소 묵살 있다 아프리카 돼지 열병 번 확진 판정 경기 연천군 농장 주 씨 30 일 오후 뉴스1 통화 멧돼지 감염 원인 생각 말 근거 씨 명 직원 최근 농장 주변 출몰 멧돼지 수차례 목격 농장 울타리 12 밑 부분 멧돼지 흔적 발견 때문 이웃 한우 농가 멧돼지 울타리 바닥 부분 흔적 발견 씨 부친 연천군 신서면 농장 마리 양주시 은현면 하 패 리 농장 마리 중 일 양성 판정 일 오전 연천 양주 개 농장 돼지 살 처분 검역 당국 농장 감염 경로 파악 상태 해당 농장 반경 예방 살 처분 위치 농장 예방 살 처분 추진 있다 씨 지난달 일 확진 판정 전날 일 이동 제한 우리 농장 외부 사람 아무도 초소 공무원 등 방역 관 농장 감시 액 비차 퇴비 차 약품 차 유일 사료 벌 대만 거점 소독 출입 유일한 바이러스 유입 원 멧돼지 다 주장 일 측 남방 한계선 전방 지점 감염 새끼 멧돼지 사체 발견 바 우리농 장과 직선 멧돼지 유입 원일 것 주장 개진 씨 멧돼지 조사 당국 수차례 건의 당국 묵살 멧돼지 언급 호소 주 전 이동 제한 방역 활동 농장 딸 얼굴 사람 것 멧돼지 것 강조 다수 양 돈 인 멧돼지 활동 반경 항공 방제 멧돼지 수 남쪽 뿐 정부 감염 매개체 의심 멧돼지 제거 집 돼지 어처구니 방역 정책 발생 암 곳 전이 수 주변 장기 식 처방 비판 다 이낙연 총리 일 방역 상황 점검 회의 예방 처분 과정 지역 속 돈 농가 이의 수 절대 다수 국민 신속 조치 지역 돈 농가 예방 처분 때 걱정 피해 최소 이상 피해 해당 지역 농민 판단 해주 길 말 편 아프리카 돼지 열병 관련 예방 살 처분 정부 일방 행정명령 수 동의 양 돈 인 사연 일 청와대 국민 청원 게시판 공감 있다 \n",
      "\n",
      "앵커 정부 군 저격수 동원 아프리카 돼지 열병 발생 민간인 출입 통제 선 부근 야생 멧돼지 사냥 멧돼지 사냥 바이러스 퇴치 성공 사례 체코 거론 팩트 와이 홍성 기자 기자 아프리카 돼지 열병 집중 발생 경기 북부 강원도 전문 엽 사 군 저격수 야생 멧돼지 사살 바이러스 확산 주범 판단 때문 체코 멧돼지 사냥 바이러스 퇴치 일부 언론 전문가 멧돼지 사냥 성공 사례 체코 4월 체코 발병 년 만 아프리카 돼지 열병 청정 국 선언 직전 체코 수의학 당국 발표 보고서 것 정반대 사냥 바이러스 박멸 방법 전제 감염 지역 멧돼지 수 얼마 파악 감염 멧돼지 총소리 놀라 바이러스 확산 수 우려 때문 사냥 건 감염 지역 격리 감염 사체 안전 제거 체코 당국 판단 진행 야생 멧돼지 체코 당국 야생 멧돼지 것 이동 차단 상태 년 동안 단계 야생 멧돼지 개체 수 시작 사냥 당장 바이러스 확산 긴급 수단 예방 차원 진행 얘기 조영석 국립생물자원관 박사 라트비아 처음 북쪽 일부 게 사냥 시작 나라 전체 돼지 게 체코 정책 멧돼지 아프리카 돼지 열병 발생 제일 것 발생 지역 사냥 금지 거 우리나라 멧돼지 사냥 민간인 통제 선 북쪽 중심 첫날 전국 멧돼지 마리 사살 포획 틀 동원 채 잡기 체코 사례 발생 지역 철조망 개체 수 방식 속전속결 셈 정부 철원 등 발병 지역 울타리 사냥 계획 여론 압박 양재 양도 농가 처음 멧돼지 무게 멧돼지 것 하루 백 르 대처 시간 생각 멧돼지 총소리 놀라 시작 이상 이동 수 사냥 과정 사람 차 피 바이러스 수 불안감 대응 과학 침착 게 전문가 지적 홍성 입 니다 저작권 자 무단 전재 배포 금지 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_article = []\n",
    "for numm in u_list:\n",
    "    f_article.append(sentence[numm])\n",
    "    #print(sentence[numm])\n",
    "    #print()\n",
    "    \n",
    "# 추출한 명사를 리스트로 만들기\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from konlpy.tag import Kkma, Twitter, Hannanum, Komoran\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "mydoclist = f_article\n",
    "\n",
    "# twitter 와 komoran이 가장 높은 유사도를 보인다.\n",
    "#kkma = Kkma()\n",
    "#twitt = Twitter()\n",
    "#hann = Hannanum()\n",
    "komo = Komoran()\n",
    "\n",
    "doc_nouns_list = [] \n",
    " \n",
    "for doc in mydoclist:\n",
    "    nouns = komo.nouns(doc)\n",
    "    doc_nouns = ''\n",
    " \n",
    "    for noun in nouns:\n",
    "        doc_nouns += noun + ' ' # 추출한 명사를 문장으로 합침\n",
    " \n",
    "    doc_nouns_list.append(doc_nouns) # doc_nouns_list에 추가\n",
    " \n",
    "#print(doc_nouns_list[1])\n",
    "for arti in doc_nouns_list:\n",
    "    print(arti)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 분류\n",
    "# 출처 : https://bangseogs.tistory.com/96\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도 분석을 위한 8x8 matrix\n",
      "  (0, 3)\t0.06485271918433826\n",
      "  (0, 1)\t0.1603533049143991\n",
      "  (0, 7)\t0.14747832556887527\n",
      "  (0, 4)\t0.10731096312091541\n",
      "  (0, 5)\t0.09123771151345962\n",
      "  (0, 6)\t0.4731250861989176\n",
      "  (0, 2)\t0.11729945885237755\n",
      "  (0, 0)\t0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "# 6 TF-IDF 모델 (유사도 분석)\n",
    "# 참조링크 : \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "mydoclist = doc_nouns_list\n",
    " \n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(mydoclist)\n",
    " \n",
    "document_distances = (tfidf_matrix * tfidf_matrix.T)\n",
    " \n",
    "print ('유사도 분석을 위한 ' + str(document_distances.get_shape()[0]) + 'x' + str(document_distances.get_shape()[1]) + ' matrix')\n",
    "\n",
    "print(document_distances[0]) # 요기서 0.2 넘는 데이터를 추린다.\n",
    "#print(document_distances.toarray())\n",
    "\n",
    "#  -   doc1 doc2 doc3\n",
    "#doc1   1\n",
    "#doc2        1\n",
    "#doc3             1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twitt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b0fee2e746f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mddoc1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mddoc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwitt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_nouns_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mddoc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'twitt' is not defined"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "ddoc1= []\n",
    "ddoc1 = twitt.nouns(doc_nouns_list[0])\n",
    "\n",
    "ddoc2 = []\n",
    "ddoc2 = twitt.nouns(doc_nouns_list[1])\n",
    "#\n",
    "print(ddoc1)\n",
    "print()\n",
    "print(ddoc2)\n",
    "#print(cos_sim(ddoc1, ddoc2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 문장의 유사도 분석하기(레벤슈타인 거리, N-gram)\n",
    "# 참조 : https://github.com/suites/actual-deeplearning/blob/master/ch6/lev-distance.py\n",
    "def calc_distance(a, b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    a_len = len(a)\n",
    "    b_len = len(b)\n",
    "    if a == \"\":\n",
    "        return b_len\n",
    "    if b == \"\":\n",
    "        return a_len\n",
    "    # 2차원 표 (a_len+1, b_len+2) 준비하기\n",
    "    matrix = [[] for i in range(a_len+1)]\n",
    "    for i in range(a_len+1):\n",
    "        matrix[i] = [0 for j in range(b_len+1)]\n",
    "    # 0일 때 초기값을 설정\n",
    "    for i in range(a_len+1):\n",
    "        matrix[i][0] = i\n",
    "    for j in range(b_len+1):\n",
    "        matrix[0][j] = j\n",
    "    # 표 채우기\n",
    "    for i in range(1, a_len+1):\n",
    "        ac = a[i-1]\n",
    "        for j in range(1, b_len+1):\n",
    "            bc = b[j-1]\n",
    "            cost = 0 if (ac == bc) else 1\n",
    "            matrix[i][j] = min([\n",
    "                matrix[i-1][j] + 1,     # 문자 삽입\n",
    "                matrix[i][j-1] + 1,     # 문자 제거\n",
    "                matrix[i-1][j-1] + cost # 문자 변경\n",
    "            ])\n",
    "    return matrix[a_len][b_len]\n",
    "\n",
    "print(calc_distance(doc_nouns_list[0], doc_nouns_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장단위 토큰화\n",
    "# https://soyoung-new-challenge.tistory.com/45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM 으로 기사 분류\n",
    "# 참조 : https://mc.ai/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%ACnlp-17%EC%9D%BC%EC%B0%A8-%EB%A1%9C%EC%9D%B4%ED%84%B0-%EB%89%B4%EC%8A%A4-%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install networkx\n",
    "\n",
    "import networkx\n",
    "import re\n",
    " \n",
    "class RawSentence:\n",
    "    def __init__(self, textIter):\n",
    "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "        else: self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield s\n",
    " \n",
    "class RawSentenceReader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield s\n",
    " \n",
    "class RawTagger:\n",
    "    def __init__(self, textIter, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Komoran\n",
    "            self.tagger = Komoran()\n",
    "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
    "        else: self.textIter = textIter\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in self.textIter:\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            if len(ch) == 1:\n",
    "                ch.append(\".\")\n",
    "                \n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n",
    " \n",
    "class RawTaggerReader:\n",
    "    def __init__(self, filepath, tagger = None):\n",
    "        if tagger:\n",
    "            self.tagger = tagger\n",
    "        else :\n",
    "            from konlpy.tag import Komoran\n",
    "            self.tagger = Komoran()\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
    "                if not s: continue\n",
    "                yield self.tagger.pos(s)\n",
    " \n",
    "class TextRank:\n",
    "    def __init__(self, **kargs):\n",
    "        self.graph = None\n",
    "        self.window = kargs.get('window', 5)\n",
    "        self.coef = kargs.get('coef', 1.0)\n",
    "        self.threshold = kargs.get('threshold', 0.005)\n",
    "        self.dictCount = {}\n",
    "        self.dictBiCount = {}\n",
    "        self.dictNear = {}\n",
    "        self.nTotal = 0\n",
    " \n",
    " \n",
    "    def load(self, sentenceIter, wordFilter = None):\n",
    "        def insertPair(a, b):\n",
    "            if a > b: a, b = b, a\n",
    "            elif a == b: return\n",
    "            self.dictBiCount[a, b] = self.dictBiCount.get((a, b), 0) + 1\n",
    " \n",
    "        def insertNearPair(a, b):\n",
    "            self.dictNear[a, b] = self.dictNear.get((a, b), 0) + 1\n",
    " \n",
    "        for sent in sentenceIter:\n",
    "            for i, word in enumerate(sent):\n",
    "                if wordFilter and not wordFilter(word): continue\n",
    "                self.dictCount[word] = self.dictCount.get(word, 0) + 1\n",
    "                self.nTotal += 1\n",
    "                if i - 1 >= 0 and (not wordFilter or wordFilter(sent[i-1])): insertNearPair(sent[i-1], word)\n",
    "                if i + 1 < len(sent) and (not wordFilter or wordFilter(sent[i+1])): insertNearPair(word, sent[i+1])\n",
    "                for j in range(i+1, min(i+self.window+1, len(sent))):\n",
    "                    if wordFilter and not wordFilter(sent[j]): continue\n",
    "                    if sent[j] != word: insertPair(word, sent[j])\n",
    " \n",
    "    def loadSents(self, sentenceIter, tokenizer = None):\n",
    "        import math\n",
    "        def similarity(a, b):\n",
    "            n = len(a.intersection(b))\n",
    "            return n / float(len(a) + len(b) - n) / (math.log(len(a)+1) * math.log(len(b)+1))\n",
    " \n",
    "        if not tokenizer: rgxSplitter = re.compile('[\\\\s.,:;-?!()\"\\']+')\n",
    "        sentSet = []\n",
    "        for sent in filter(None, sentenceIter):\n",
    "            if type(sent) == str:\n",
    "                if tokenizer: s = set(filter(None, tokenizer(sent)))\n",
    "                else: s = set(filter(None, rgxSplitter.split(sent)))\n",
    "            else: s = set(sent)\n",
    "            if len(s) < 2: continue\n",
    "            self.dictCount[len(self.dictCount)] = sent\n",
    "            sentSet.append(s)\n",
    " \n",
    "        for i in range(len(self.dictCount)):\n",
    "            for j in range(i+1, len(self.dictCount)):\n",
    "                s = similarity(sentSet[i], sentSet[j])\n",
    "                if s < self.threshold: continue\n",
    "                self.dictBiCount[i, j] = s\n",
    " \n",
    "    def getPMI(self, a, b):\n",
    "        import math\n",
    "        co = self.dictNear.get((a, b), 0)\n",
    "        if not co: return None\n",
    "        return math.log(float(co) * self.nTotal / self.dictCount[a] / self.dictCount[b])\n",
    " \n",
    "    def getI(self, a):\n",
    "        import math\n",
    "        if a not in self.dictCount: return None\n",
    "        return math.log(self.nTotal / self.dictCount[a])\n",
    " \n",
    "    def build(self):\n",
    "        self.graph = networkx.Graph()\n",
    "        self.graph.add_nodes_from(self.dictCount.keys())\n",
    "        for (a, b), n in self.dictBiCount.items():\n",
    "            self.graph.add_edge(a, b, weight=n*self.coef + (1-self.coef))\n",
    " \n",
    "    def rank(self):\n",
    "        return networkx.pagerank(self.graph, weight='weight')\n",
    " \n",
    "    def extract(self, ratio = 0.1):\n",
    "        ranks = self.rank()\n",
    "        cand = sorted(ranks, key=ranks.get, reverse=True)[:int(len(ranks) * ratio)]\n",
    "        pairness = {}\n",
    "        startOf = {}\n",
    "        tuples = {}\n",
    "        for k in cand:\n",
    "            tuples[(k,)] = self.getI(k) * ranks[k]\n",
    "            for l in cand:\n",
    "                if k == l: continue\n",
    "                pmi = self.getPMI(k, l)\n",
    "                if pmi: pairness[k, l] = pmi\n",
    " \n",
    "        for (k, l) in sorted(pairness, key=pairness.get, reverse=True):\n",
    "            print(k[0], l[0], pairness[k, l])\n",
    "            if k not in startOf: startOf[k] = (k, l)\n",
    " \n",
    "        for (k, l), v in pairness.items():\n",
    "            pmis = v\n",
    "            rs = ranks[k] * ranks[l]\n",
    "            path = (k, l)\n",
    "            tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    "            last = l\n",
    "            while last in startOf and len(path) < 7:\n",
    "                if last in path: break\n",
    "                pmis += pairness[startOf[last]]\n",
    "                last = startOf[last][1]\n",
    "                rs *= ranks[last]\n",
    "                path += (last,)\n",
    "                tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
    " \n",
    "        used = set()\n",
    "        both = {}\n",
    "        for k in sorted(tuples, key=tuples.get, reverse=True):\n",
    "            if used.intersection(set(k)): continue\n",
    "            both[k] = tuples[k]\n",
    "            for w in k: used.add(w)\n",
    " \n",
    "        #for k in cand:\n",
    "        #    if k not in used or True: both[k] = ranks[k] * self.getI(k)\n",
    " \n",
    "        return both\n",
    " \n",
    "    def summarize(self, ratio = 0.333):\n",
    "        r = self.rank()\n",
    "        ks = sorted(r, key=r.get, reverse=True)[:int(len(r)*ratio)]\n",
    "        return ' '.join(map(lambda k:self.dictCount[k], sorted(ks)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load...\n",
      "Build...\n",
      "0\t1.0\t[('앵커', 'NNP'), ('야생', 'NNG'), ('멧돼지', 'NNP'), ('아프리카', 'NNP'), ('돼지', 'NNP'), ('열병', 'NNG'), ('게', 'NNG'), ('확인', 'NNG'), ('그동안', 'NNG'), ('멧돼지', 'NNP'), ('신경', 'NNG'), ('당국', 'NNG'), ('비판', 'NNG'), ('멧돼지', 'NNP'), ('열병', 'NNG'), ('동유럽', 'NNP'), ('체코', 'NNP'), ('사례', 'NNG'), ('참', 'VV'), ('고', 'EC'), ('필요', 'NNG'), ('말', 'NNG'), ('김', 'NNP'), ('관', 'NNG'), ('지', 'VX'), ('ㄴ', 'ETM'), ('기자', 'NNG'), ('입', 'NNG'), ('니다', 'NNP'), ('기자', 'NNG'), ('지나', 'VV'), ('ㄴ', 'ETM'), ('5월', 'NNP'), ('북한', 'NNP'), ('아프리카', 'NNP'), ('돼지', 'NNP'), ('열병', 'NNG'), ('발병', 'NNG'), ('환경부', 'NNP'), ('접경', 'NNG'), ('지', 'NNB'), ('돼지', 'NNP'), ('농가', 'NNP'), ('주변', 'NNG'), ('울타리', 'NNP'), ('설치', 'NNG'), ('등', 'NNB'), ('차단', 'NNP'), ('방역', 'NNP'), ('적극', 'NNG'), ('정작', 'NNG'), ('국내', 'NNG'), ('확진', 'NNG'), ('사례', 'NNG'), ('후', 'NNG'), ('환경부', 'NNP'), ('멧돼지', 'NNP'), ('소극', 'NNP'), ('대응', 'NNG'), ('집', 'NNG'), ('돼지', 'NNP'), ('잡기', 'NNP'), ('집중', 'NNG'), ('동안', 'NNG'), ('야생', 'NNG'), ('멧돼지', 'NNP'), ('사체', 'NNG'), ('아프리카', 'NNP'), ('돼지', 'NNP'), ('열병', 'NNG'), ('바이러스', 'NNP'), ('검출', 'NNG'), ('확인', 'NNG'), ('전문가', 'NNG'), ('내부', 'NNG'), ('바이러스', 'NNP'), ('기합', 'NNP'), ('니다', 'NNP'), ('파리', 'NNP'), ('고양이', 'NNP'), ('진드기', 'NNP'), ('등', 'NNB'), ('감염', 'NNP'), ('멧돼지', 'NNP'), ('사체', 'NNG'), ('배설물', 'NNG'), ('접촉', 'NNG'), ('때', 'NNG'), ('바이러스', 'NNP'), ('수', 'NNB'), ('우', 'NNP'), ('희종', 'NNP'), ('교수', 'NNG'), ('서울대', 'NNP'), ('수의학', 'NNP'), ('북', 'NNP'), ('측', 'NNB'), ('남', 'NNP'), ('측', 'NNB'), ('경계선', 'NNG'), ('사이', 'NNG'), ('에', 'JKB'), ('남', 'NNP'), ('발생', 'NNG'), ('북', 'NNP'), ('발생', 'NNG'), ('말', 'NNG'), ('건', 'NNB'), ('검역', 'NNP'), ('본부', 'NNG'), ('최근', 'NNG'), ('체코', 'NNP'), ('아프리카', 'NNP'), ('돼지', 'NNP'), ('열병', 'NNG'), ('방역', 'NNP'), ('사례', 'NNG'), ('주목', 'NNG'), ('것', 'NNB'), ('체', 'NNB'), ('코', 'NNG'), ('병', 'NNG'), ('유입', 'NNG'), ('건', 'NNB'), ('년', 'NNB'), ('이후', 'NNG'), ('2', 'SN'), ('년', 'NNB'), ('간', 'NNB'), ('건', 'NNB'), ('발병', 'NNG'), ('올해', 'NNG'), ('건의', 'NNP'), ('확진', 'NNG'), ('때문', 'NNB'), ('최근', 'NNG'), ('3', 'SN'), ('년', 'NNB'), ('간', 'NNB'), ('아프리카', 'NNP'), ('돼지', 'NNP'), ('열병', 'NNG'), ('발병', 'NNG'), ('개국', 'NNB'), ('가운데', 'NNG'), ('체코', 'NNP'), ('기간', 'NNG'), ('바이러스', 'NNP'), ('박멸', 'NNG'), ('성공', 'NNG'), ('국가', 'NNG'), ('초기', 'NNG'), ('집중', 'NNG'), ('야생', 'NNG'), ('멧돼지', 'NNP'), ('포획', 'NNG'), ('사살', 'NNP'), ('대책', 'NNG'), ('주효', 'NNG'), ('평가', 'NNG'), ('입', 'NNG'), ('니다', 'NNP'), ('경찰', 'NNG'), ('군대', 'NNP'), ('동원', 'NNG'), ('멧돼지', 'NNP'), ('개체', 'NNP'), ('수', 'NNB'), ('관리', 'NNG'), ('사냥', 'NNP'), ('보상금', 'NNG'), ('체코', 'NNP'), ('야산', 'NNG'), ('야생', 'NNG'), ('멧돼지', 'NNP'), ('사살', 'NNP'), ('수렵', 'NNG'), ('인', 'NNG'), ('활동', 'NNG'), ('정현', 'NNP'), ('규', 'NNG'), ('양', 'MM'), ('돈', 'NNG'), ('수의사', 'NNP'), ('협회장', 'NNG'), ('체코', 'NNP'), ('멧돼지', 'NNP'), ('일정', 'NNG'), ('지역', 'NNG'), ('절멸', 'NNP'), ('시키', 'NNP'), ('작업', 'NNG'), ('작업', 'NNG'), ('대책', 'NNG'), ('지금', 'MAG'), ('노력', 'NNG'), ('수', 'NNB'), ('포로', 'NNP'), ('관리', 'NNG'), ('멧돼지', 'NNP'), ('바이러스', 'NNP'), ('만연', 'NNG'), ('경우', 'NNG'), ('토착화', 'NNP'), ('우려', 'NNG'), ('만큼', 'NNB'), ('대책', 'NNG'), ('필요', 'NNG'), ('영상', 'NNP'), ('취재', 'NNG'), ('서진', 'NNP'), ('호', 'NNB'), ('영상 편집', 'NNP'), ('황', 'NNP'), ('지영', 'NNP'), ('화면', 'NNG'), ('출처', 'NNG'), ('유튜브', 'NNP'), ('자료', 'NNG'), ('제공', 'NNG'), ('더불어민주당', 'NNP'), ('김현', 'NNP'), ('의원', 'NNG'), ('실', 'NNG'), ('김관진', 'NNP'), ('기자', 'NNG'), ('.', 'SF')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr = TextRank()\n",
    "print('Load...')\n",
    "from konlpy.tag import Komoran\n",
    "tagger = Komoran()\n",
    "stopword = set([('있', 'VV'), ('하', 'VV'), ('되', 'VV') ])\n",
    "#tr.loadSents(doc_nouns_list[0], lambda sent: filter(lambda x:x not in stopword and x[1] in ('NNG', 'NNP', 'VV', 'VA'), tagger.pos(sent)))\n",
    "tr.loadSents(RawTagger(doc_nouns_list[1]), lambda sent: filter(lambda x:x not in stopword and x[1] in ('NNG', 'NNP', 'VV', 'VA'), tagger.pos(sent)))\n",
    "# RawTagger(\"특정 텍스트....\")\n",
    "#tr = doc_nouns_list[0]\n",
    "print('Build...')\n",
    "tr.build()\n",
    "ranks = tr.rank()\n",
    "for k in sorted(ranks, key=ranks.get, reverse=True)[:100]:\n",
    "    print(\"\\t\".join([str(k), str(ranks[k]), str(tr.dictCount[k])]))\n",
    "    \n",
    "print(tr.summarize(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load...\n",
      "Build...\n",
      "신고 접수 4.394449154672439\n",
      "의심 신고 4.212127597878484\n",
      "북한 멧돼지 3.7013019741124933\n",
      "돼지 농장 3.0081547935525483\n",
      "지역 의심 3.0081547935525483\n",
      "포천 북한 2.720472721100767\n",
      "경우 포천 2.720472721100767\n",
      "정부 방역 2.63086056241108\n",
      "포천 의심 2.315007612992603\n",
      "접수 포천 2.315007612992603\n",
      "포천 방역 2.1608569331653444\n",
      "접수 농장 2.0918640616783932\n",
      "농장 돼지 1.3987168811184478\n",
      "(('돼지', 'NNP'), ('농장', 'NNP'))\t0.207457\n",
      "(('신고', 'NNP'), ('접수', 'NNG'))\t0.182408\n",
      "(('포천', 'NNP'), ('북한', 'NNP'))\t0.115707\n",
      "(('정부', 'NNG'), ('방역', 'NNP'))\t0.111596\n",
      "(('지역', 'NNG'), ('의심', 'NNG'))\t0.10454\n",
      "(('경기', 'NNP'),)\t0.0641438\n",
      "(('경우', 'NNG'),)\t0.0616773\n",
      "(('멧돼지', 'NNP'),)\t0.0586824\n"
     ]
    }
   ],
   "source": [
    "top_key = []\n",
    "\n",
    "tr = TextRank(window=5, coef=1)\n",
    "print('Load...')\n",
    "stopword = set([('있', 'VV'), ('하', 'VV'), ('되', 'VV'), ('없', 'VV') ])\n",
    "tr.load(RawTagger(doc_nouns_list[0]), lambda w: w not in stopword and (w[1] in ('NNG', 'NNP', 'VV', 'VA')))\n",
    "print('Build...')\n",
    "tr.build()\n",
    "kw = tr.extract(0.1)\n",
    "for k in sorted(kw, key=kw.get, reverse=True):\n",
    "    print(\"%s\\t%g\" % (k, kw[k]))\n",
    "    top_key.append(\"%s %g\" % (k, kw[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05868244039230347\n",
      "(('멧돼지', 'NNP'),)\n",
      "멧돼지\n",
      "top_key\n",
      "[\"(('돼지', 'NNP'), ('농장', 'NNP')) 0.207457\", \"(('신고', 'NNP'), ('접수', 'NNG')) 0.182408\", \"(('포천', 'NNP'), ('북한', 'NNP')) 0.115707\", \"(('정부', 'NNG'), ('방역', 'NNP')) 0.111596\", \"(('지역', 'NNG'), ('의심', 'NNG')) 0.10454\", \"(('경기', 'NNP'),) 0.0641438\", \"(('경우', 'NNG'),) 0.0616773\", \"(('멧돼지', 'NNP'),) 0.0586824\"]\n"
     ]
    }
   ],
   "source": [
    "print(kw[k])\n",
    "print(k)\n",
    "print(k[0][0])\n",
    "print(\"top_key\")\n",
    "print(top_key) # article 인덱스별 상위 키워드 탑 키에서 돼지 열병 관련 키워드를 골라 인덱스 선별, 선별한 기사 목록에서 지명만 추출, 지명 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load...\n",
      "Build...\n",
      "신고 접수 4.394449154672439\n",
      "의심 신고 4.212127597878484\n",
      "북한 멧돼지 3.7013019741124933\n",
      "돼지 농장 3.0081547935525483\n",
      "지역 의심 3.0081547935525483\n",
      "포천 북한 2.720472721100767\n",
      "경우 포천 2.720472721100767\n",
      "정부 방역 2.63086056241108\n",
      "포천 의심 2.315007612992603\n",
      "접수 포천 2.315007612992603\n",
      "포천 방역 2.1608569331653444\n",
      "접수 농장 2.0918640616783932\n",
      "농장 돼지 1.3987168811184478\n",
      "(('돼지', 'NNP'), ('농장', 'NNP'))\t0.207457\n",
      "(('신고', 'NNP'), ('접수', 'NNG'))\t0.182408\n",
      "(('포천', 'NNP'), ('북한', 'NNP'))\t0.115707\n",
      "(('정부', 'NNG'), ('방역', 'NNP'))\t0.111596\n",
      "(('지역', 'NNG'), ('의심', 'NNG'))\t0.10454\n",
      "(('경기', 'NNP'),)\t0.0641438\n",
      "(('경우', 'NNG'),)\t0.0616773\n",
      "(('멧돼지', 'NNP'),)\t0.0586824\n",
      "\n",
      "Load...\n",
      "Build...\n",
      "사살 대책 4.174387269895637\n",
      "아프리카 돼지 4.020236590068379\n",
      "돼지 열병 3.8379150332744243\n",
      "열병 발병 3.7689221617874726\n",
      "니다 기자 3.7689221617874726\n",
      "야생 멧돼지 3.5682514663253215\n",
      "체코 사례 3.258096538021482\n",
      "멧돼지 사살 2.875104285765376\n",
      "체코 아프리카 2.747270914255491\n",
      "열병 바이러스 2.5649493574615367\n",
      "멧돼지 바이러스 1.9588135538912212\n",
      "멧돼지 아프리카 1.9588135538912212\n",
      "체코 멧돼지 1.9588135538912212\n",
      "멧돼지 열병 1.7764919970972666\n",
      "(('야생', 'NNG'), ('멧돼지', 'NNP'))\t0.212912\n",
      "(('돼지', 'NNP'), ('열병', 'NNG'))\t0.206074\n",
      "(('체코', 'NNP'), ('사례', 'NNG'))\t0.12079\n",
      "(('사살', 'NNP'), ('대책', 'NNG'))\t0.104952\n",
      "(('니다', 'NNP'), ('기자', 'NNG'))\t0.0926904\n",
      "(('바이러스', 'NNP'),)\t0.0847263\n",
      "(('아프리카', 'NNP'),)\t0.0737322\n",
      "(('발병', 'NNG'),)\t0.0489815\n",
      "\n",
      "Load...\n",
      "Build...\n",
      "시민 불안 5.032614200815031\n",
      "도심 출몰 4.1853163404278275\n",
      "출몰 시민 3.9340019121469214\n",
      "신고 경찰 3.6463198396951406\n",
      "경찰 신고 3.6463198396951406\n",
      "주택가 멧돼지 2.5902671654458267\n",
      "멧돼지 사살 1.8971199848858813\n",
      "멧돼지 주차장 1.8971199848858813\n",
      "멧돼지 시민 1.8971199848858813\n",
      "멧돼지 공격 1.8971199848858813\n",
      "멧돼지 포획 1.742969305058623\n",
      "부산 멧돼지 1.3862943611198906\n",
      "사살 멧돼지 1.2039728043259361\n",
      "멧돼지 도심 1.0498221244986776\n",
      "포획 멧돼지 1.0498221244986776\n",
      "(('멧돼지', 'NNP'),)\t0.187618\n",
      "(('도심', 'NNP'), ('출몰', 'NNG'))\t0.134011\n",
      "(('시민', 'NNG'), ('불안', 'NNG'))\t0.11131\n",
      "(('신고', 'NNP'), ('경찰', 'NNG'))\t0.0976514\n",
      "(('포획', 'NNG'),)\t0.0905407\n",
      "(('사살', 'NNP'),)\t0.0805454\n",
      "(('부산', 'NNP'),)\t0.0702936\n",
      "(('주차장', 'NNP'),)\t0.0565073\n",
      "(('서식', 'NNG'),)\t0.0506325\n",
      "(('주택가', 'NNG'),)\t0.0484574\n",
      "(('공격', 'NNG'),)\t0.0439255\n",
      "\n",
      "Load...\n",
      "Build...\n",
      "입 니다 3.871201010907891\n",
      "멧돼지 미터 2.618438042412523\n",
      "앞바다 멧돼지 2.618438042412523\n",
      "멧돼지 입 2.2129729343043585\n",
      "(('입', 'NNG'), ('니다', 'NNP'))\t0.263312\n",
      "(('앞바다', 'NNG'), ('멧돼지', 'NNP'))\t0.200302\n",
      "(('바다', 'NNG'),)\t0.112589\n",
      "(('배', 'NNG'),)\t0.0829336\n",
      "(('미터', 'NNP'),)\t0.0815397\n",
      "\n",
      "Load...\n",
      "Build...\n",
      "시간 동안 5.168588259873252\n",
      "바이러스 확산 4.763123151765088\n",
      "엽 사 4.657762636107262\n",
      "돼지 열병 4.657762636107262\n",
      "아프리카 돼지 4.657762636107262\n",
      "전병호 엽 4.475441079313307\n",
      "사 보상금 4.252297527999097\n",
      "열병 확산 4.069975971205142\n",
      "열병 바이러스 4.069975971205142\n",
      "설치 전병호 4.069975971205142\n",
      "추적 사냥개 3.6281432189261036\n",
      "사 전병호 3.5591503474391524\n",
      "우려 엽 3.5591503474391524\n",
      "안전 사냥 3.3768287906451975\n",
      "추적 전병호 3.2226781108179394\n",
      "야생 멧돼지 3.0891467181934167\n",
      "돼지 사람 3.048324723673162\n",
      "사람 돼지 3.048324723673162\n",
      "수색 활동 3.028522096376982\n",
      "발견 야생 3.0091040105198803\n",
      "엽 사냥개 2.866003166879207\n",
      "멧돼지 사냥 2.683681610085252\n",
      "안전 야생 2.6036389024117157\n",
      "사냥개 추적 2.529530930257994\n",
      "사 수색 2.517696472610991\n",
      "사냥개 엽 2.1728559863192616\n",
      "사냥개 돼지 2.1728559863192616\n",
      "멧돼지 이동 1.990534429525307\n",
      "멧돼지 확산 1.990534429525307\n",
      "멧돼지 동안 1.990534429525307\n",
      "온몸 멧돼지 1.990534429525307\n",
      "사냥개 야생 1.9104917218517705\n",
      "멧돼지 추적 1.8363837496980486\n",
      "멧돼지 발견 1.7028523570735261\n",
      "사냥개 수색 1.642227735257091\n",
      "멧돼지 포획 1.4797088057593164\n",
      "사람 멧돼지 1.4797088057593164\n",
      "멧돼지 수색 1.3545456628053103\n",
      "사냥개 멧돼지 1.2973872489653615\n",
      "수색 멧돼지 0.2559333741372007\n",
      "(('야생', 'NNG'), ('멧돼지', 'NNP'))\t0.24591\n",
      "(('아프리카', 'NNP'), ('돼지', 'NNP'))\t0.116285\n",
      "(('전병호', 'NNP'), ('엽', 'NNG'))\t0.106574\n",
      "(('추적', 'NNP'), ('사냥개', 'NNP'))\t0.0991545\n",
      "(('수색', 'NNP'),)\t0.0845909\n",
      "(('시간', 'NNG'), ('동안', 'NNG'))\t0.0643076\n",
      "(('열병', 'NNG'), ('확산', 'NNG'))\t0.0610045\n",
      "(('사', 'NNG'), ('보상금', 'NNG'))\t0.0557502\n",
      "(('안전', 'NNG'), ('사냥', 'NNP'))\t0.0547737\n",
      "(('포획', 'NNG'),)\t0.04108\n",
      "(('발견', 'NNG'),)\t0.0347045\n",
      "(('사람', 'NNG'),)\t0.0326806\n",
      "(('이동', 'NNP'),)\t0.0309319\n",
      "(('온몸', 'NNG'),)\t0.0308356\n",
      "(('활동', 'NNG'),)\t0.0298968\n",
      "(('메리', 'NNP'),)\t0.0291943\n",
      "(('우려', 'NNG'),)\t0.0288504\n",
      "(('설치', 'NNG'),)\t0.0282923\n",
      "(('어둠', 'NNP'),)\t0.0282559\n",
      "(('바이러스', 'NNP'),)\t0.0275547\n",
      "(('밭', 'NNG'),)\t0.0274942\n",
      "\n",
      "Load...\n",
      "Build...\n",
      "차량 사람 3.7054087560651467\n",
      "추정 차량 3.7054087560651467\n",
      "멧돼지 출몰 2.406125771934886\n",
      "멧돼지 차량 2.000660663826722\n",
      "차량 멧돼지 2.0006606638267215\n",
      "(('멧돼지', 'NNP'), ('출몰', 'NNG'))\t0.176083\n",
      "(('차량', 'NNG'), ('사람', 'NNG'))\t0.131856\n",
      "(('울산', 'NNP'),)\t0.0961068\n",
      "(('관리', 'NNG'),)\t0.0720757\n",
      "(('니다', 'NNP'),)\t0.0705909\n",
      "(('앞', 'NNG'),)\t0.0678501\n",
      "(('추정', 'NNP'),)\t0.0643758\n",
      "\n",
      "Load...\n",
      "Build...\n",
      "돼지 열병 4.490240219314114\n",
      "아프리카 돼지 4.490240219314114\n",
      "예방 살 4.315886832169337\n",
      "돈 농가 3.9794145955481235\n",
      "확진 농가 3.797093038754169\n",
      "지역 돈 3.728100167267218\n",
      "방역 당국 3.5739494874399593\n",
      "예방 처분 3.5739494874399593\n",
      "열병 확진 3.258096538021482\n",
      "농장 울타리 3.1039458581942236\n",
      "돼지 아프리카 3.1039458581942236\n",
      "농가 예방 2.880802306880014\n",
      "돼지 살 2.880802306880014\n",
      "농가 방역 2.544330070258801\n",
      "농장 감염 2.4107986776342782\n",
      "멧돼지 감염 2.216642663193321\n",
      "멧돼지 울타리 2.216642663193321\n",
      "발견 멧돼지 2.216642663193321\n",
      "농장 예방 2.1876551263200685\n",
      "당국 농장 2.1876551263200685\n",
      "농장 돼지 1.8511828896988558\n",
      "농가 멧돼지 1.6570268752578983\n",
      "멧돼지 농장 0.963879694697953\n",
      "(('돈', 'NNG'), ('농가', 'NNP'))\t0.134628\n",
      "(('멧돼지', 'NNP'),)\t0.133998\n",
      "(('방역', 'NNP'), ('당국', 'NNG'))\t0.130462\n",
      "(('농장', 'NNP'), ('울타리', 'NNP'))\t0.130327\n",
      "(('예방', 'NNP'), ('살', 'VV'))\t0.124479\n",
      "(('아프리카', 'NNP'), ('돼지', 'NNP'))\t0.121124\n",
      "(('열병', 'NNG'), ('확진', 'NNG'))\t0.0787492\n",
      "(('처분', 'NNG'),)\t0.0756245\n",
      "(('감염', 'NNP'),)\t0.0541173\n",
      "(('발견', 'NNG'),)\t0.0515106\n",
      "(('지역', 'NNG'),)\t0.0481353\n",
      "\n",
      "Load...\n",
      "Build...\n",
      "돼지 열병 4.5217885770490405\n",
      "아프리카 돼지 4.5217885770490405\n",
      "열병 발생 3.828641396489095\n",
      "감염 지역 3.828641396489095\n",
      "발생 지역 3.6054978451748854\n",
      "돼지 게 3.4231762883809305\n",
      "야생 멧돼지 3.3586377672433594\n",
      "게 체코 2.835389623478812\n",
      "사냥 바이러스 2.7990219793079367\n",
      "사냥 시작 2.5477075510270306\n",
      "게 사냥 2.5477075510270306\n",
      "시작 사냥 2.5477075510270306\n",
      "멧돼지 사냥 2.4831690298894595\n",
      "지역 사냥 2.03688192726104\n",
      "멧돼지 아프리카 1.9723434061234688\n",
      "감염 멧돼지 1.9723434061234688\n",
      "지역 멧돼지 1.749199854809259\n",
      "멧돼지 체코 1.16141318990714\n",
      "체코 멧돼지 1.16141318990714\n",
      "사냥 멧돼지 0.8737311174553593\n",
      "(('멧돼지', 'NNP'), ('사냥', 'NNP'))\t0.243824\n",
      "(('아프리카', 'NNP'), ('돼지', 'NNP'))\t0.162718\n",
      "(('발생', 'NNG'), ('지역', 'NNG'))\t0.133135\n",
      "(('게', 'NNG'), ('체코', 'NNP'))\t0.121296\n",
      "(('바이러스', 'NNP'),)\t0.0938466\n",
      "(('야생', 'NNG'),)\t0.0723735\n",
      "(('열병', 'NNG'),)\t0.0650201\n",
      "(('감염', 'NNP'),)\t0.0579944\n",
      "(('시작', 'NNG'),)\t0.0553911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for arti in doc_nouns_list:\n",
    "    tr = TextRank(window=5, coef=1)\n",
    "    print('Load...')\n",
    "    stopword = set([('있', 'VV'), ('하', 'VV'), ('되', 'VV'), ('없', 'VV') ])\n",
    "    tr.load(RawTagger(arti), lambda w: w not in stopword and (w[1] in ('NNG', 'NNP', 'VV', 'VA')))\n",
    "    print('Build...')\n",
    "    tr.build()\n",
    "    kw = tr.extract(0.1)\n",
    "    for k in sorted(kw, key=kw.get, reverse=True):\n",
    "        print(\"%s\\t%g\" % (k, kw[k]))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
