{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Json 등 형식에 맞는 파일 불러오기\\n2. article text를 추출\\n3. stop word 적용\\n4. text rank 핵심 문장으로 요약\\n5. //konply에서 명사만 추출 ex) okt.nouns(string)\\n6. 키워드 추출 (ex: TF-IDF)\\n7. 아니면 핵심 문장간 유사도 비교\\n8. 합당한 문장에서 지명 대조\\n9. 추출한 지명 따로 저장\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Json 등 형식에 맞는 파일 불러오기\n",
    "2. article text를 추출\n",
    "3. stop word 적용\n",
    "4. text rank 핵심 문장으로 요약\n",
    "5. //konply에서 명사만 추출 ex) okt.nouns(string)\n",
    "6. 키워드 추출 (ex: TF-IDF)\n",
    "7. 아니면 핵심 문장간 유사도 비교\n",
    "8. 합당한 문장에서 지명 대조\n",
    "9. 추출한 지명 따로 저장\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파주시, 아프리카돼지열병으로 인해 지역경제 유례없는 침체기\n",
      "[일요서울｜파주 강동기 기자] 파주시는 아프리카돼지열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다.15일 파주시는 대규모 점포인 ㈜신세계사이먼과 협약 체결을 통해 소상공인들을 위한 1억 원의 특례보증 출연금을 이끌어내어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다. 협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소상공인들이 1억 원의 10배수인 10억 원(50여 명)의 대출\n",
      "2019-11-15T14:40:55+09:00\n"
     ]
    }
   ],
   "source": [
    "# 1. Json File 불러오기\n",
    "import json\n",
    "\n",
    "# with를 이용해 파일을 연다.\n",
    "# json 파일은 같은 폴더에 있다고 가정!\n",
    "\n",
    "with open('news_sample.json', encoding='UTF-8') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "    # 문자열\n",
    "    # key가 json_string인 문자열 가져오기\n",
    "    json_title = json_data[\"title\"]\n",
    "    print(json_title)\n",
    "    \n",
    "# 2. Article Text 추출\n",
    "\n",
    "    # 문자열 \n",
    "    # key가 json_string인 문자열 가져오기\n",
    "    json_body = json_data[\"body\"]\n",
    "    print(json_body)\n",
    "\n",
    "    # 문자열\n",
    "    # key가 json_string인 문자열 가져오기\n",
    "    json_date = json_data[\"date\"]\n",
    "    print(json_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_body 원문\n",
      "[일요서울｜파주 강동기 기자] 파주시는 아프리카돼지열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다.15일 파주시는 대규모 점포인 ㈜신세계사이먼과 협약 체결을 통해 소상공인들을 위한 1억 원의 특례보증 출연금을 이끌어내어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다. 협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소상공인들이 1억 원의 10배수인 10억 원(50여 명)의 대출\n",
      "\n",
      "특수문자 제거 json_body (sentence)\n",
      "[' 일요서울｜파주 강동기 기자  파주시는 아프리카돼지열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다 15일 파주시는 대규모 점포인 ㈜신세계사이먼과 협약 체결을 통해 소상공인들을 위한 1억 원의 특례보증 출연금을 이끌어내어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다  협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소상공인들이 1억 원의 10배수인 10억 원 50여 명 의 대출']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3_1. 특수문자 제거\n",
    "\n",
    "# 소문자 변환\n",
    "# re 라이브러리로 특수문자 제거\n",
    "\n",
    "# 읽어온 데이터 확인 (json_body)\n",
    "print(\"json_body 원문\")\n",
    "print(json_body)\n",
    "print()\n",
    "sentence = []\n",
    "sentence.append(json_body.strip()) \n",
    "\n",
    "# 특수문자 제거 라이브러리\n",
    "import re\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    sentence[i] = sentence[i].lower() # lower() : 모든 문자를 소문자로 변환\n",
    "    sentence[i] = re.sub(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\", ' ', sentence[i]) # 특수문자 제거\n",
    "    sentence[i] = re.sub(\"[.;:!\\'?,\\\"()|[\\]]\", ' ', sentence[i]) # 특수문자 제거\n",
    "\n",
    "print(\"특수문자 제거 json_body (sentence)\")\n",
    "print(sentence)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3_2. 외부 Stop Word 리스트 불러오기 (stop word ranking 기반) \n",
    "\n",
    "# 불용어 처리 - stop_word 리스트 불러오기 & 전처리\n",
    "stop_words = []\n",
    "f = open(\"C:\\Python\\source\\stop_word.txt\", 'r', encoding='UTF-8')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    #print(line) #정상 출력 확인\n",
    "    stop_words.append(line.rstrip('\\n')) # 개행문자 제거 후 추가\n",
    "f.close()\n",
    "\n",
    "# 불러온 stop word 리스트 출력\n",
    "#print(stop_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop word 필터링 결과 (result)\n",
      "[' 일요서울｜파주 강동기 기자  파주시는 아프리카돼지열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다 15일 파주시는 대규모 점포인 ㈜신세계사이먼과 협약 체결을 통해 소상공인들을 위한 1억 원의 특례보증 출연금을 이끌어내어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다  협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소상공인들이 1억 원의 10배수인 10억 원 50여 명 의 대출']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3_3. 불러온 Stop Word 적용  \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "result = [] \n",
    "for w in sentence: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "# 위의 4줄은 아래의 한 줄로 대체 가능\n",
    "# result=[word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "# 불용어 처리 결과 출력\n",
    "print(\"stop word 필터링 결과 (result)\")\n",
    "print(result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\venv\\tensorflow\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['일요', '서울', '파주', '강', '동기', '기자', '파주시', '아프리카', '돼지', '열병', '다수', '가을', '축제', '행사', '취소', '지역', '경제', '유례', '침체', '상황', '소상', '공인', '경영', '극복', '위해', '책', '파주시', '대규모', '점포', '신세계', '사이먼', '협약', '체결', '통해', '소상', '공인', '위', '원', '특례', '보증', '출연', '금', '대규모', '점포', '지역', '내', '중소', '상인', '상생', '발전', '위', '토대', '마련', '협약', '동시', '담보', '능력', '창업', '어려움', '난', '상공', '원', '배수', '원', '여', '명', '의', '대출']\n"
     ]
    }
   ],
   "source": [
    "# 4. Text Rank로 문장 요약하기\n",
    "# 참고 링크 : https://github.com/wnsgml972/TextRank-Doc-Summary\n",
    "\n",
    "# 5. Konlpy 추가, Okt 사용, 명사만 추출 (전처리 과정)\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "nouns = []\n",
    "\n",
    "# 명사 추출에 집어넣기 좋게 텍스트로 변환\n",
    "strr = ''.join(result)\n",
    "\n",
    " \n",
    "# 명사만 추출하기 (텍스트만 받음)\n",
    "nouns = okt.nouns(strr)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\python\\venv\\tensorflow\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (1.25.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (1.17.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorflow) (0.33.6)\n",
      "Requirement already satisfied: h5py in c:\\python\\venv\\tensorflow\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (41.6.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.25.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\python\\venv\\tensorflow\\lib\\site-packages (1.17.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\python\\venv\\tensorflow\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from matplotlib) (2.4.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\python\\venv\\tensorflow\\lib\\site-packages (from matplotlib) (1.17.1)\n",
      "Requirement already satisfied: six in c:\\python\\venv\\tensorflow\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\python\\venv\\tensorflow\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (41.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six in c:\\python\\venv\\tensorflow\\lib\\site-packages (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\python\\venv\\tensorflow\\lib\\site-packages (4.39.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# 6 TF-IDF 모델 생성 및 그래프 생성\n",
    "#!pip install sklearn\n",
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install six\n",
    "!pip install tqdm\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Twitter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.kkma = Kkma()\n",
    "        self.twitter = Twitter()\n",
    "        self.stopwords = [\"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\"\n",
    "                          , \"을\", \"를\", \"에\", \"의\", \"가\",'중인' ,'만큼', '마찬가지']\n",
    "        \n",
    "    def text2sentences(self, text):\n",
    "        sentences = self.kkma.sentences(text)      \n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        return sentences\n",
    "    \n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence is not '':\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence)) \n",
    "                #nouns.append(' '.join([noun for noun in self.twitter.nouns(sentence) \n",
    "                                       if noun not in self.stopwords and len(noun) > 1]))\n",
    "        return nouns\n",
    "    \n",
    "\"\"\"\n",
    "# 6_1 matrix 생성\n",
    "tfidf = TfidfVectorizer()\n",
    "cnt_vec = CountVectorizer()\n",
    "graph_sentence = []\n",
    "\n",
    "# build_sent_graph\n",
    "tfidf_mat = tfidf.fit_transform(str).toarray()\n",
    "graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "    # return self.graph_sentence\n",
    "    \n",
    "# build_words_graph\n",
    "cnt_vec_mat = normalize(cnt_vec.fit_transform(str).toarray().astype(float), axis=0)\n",
    "vocab = cnt_vec.vocabulary_\n",
    "    #return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\"\"\"\n",
    "\n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "        \n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    \n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6_2 TextRank 알고리즘 적용\n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): \n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 \n",
    "            link_sum = np.sum(A[:,id])\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) \n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6_3 TextRank Class 구현\n",
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        \n",
    "        print(\"text\")\n",
    "        print(text)\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        \n",
    "        self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        print(\"sentences\")\n",
    "        print(self.sentences)\n",
    "        self.nouns = self.sent_tokenize.get_nouns(text)\n",
    "        \n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "    \n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        return summary\n",
    "\n",
    "    def keywords(self, word_num=10):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        \n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "\n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "\n",
    "        return keywords\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[일요서울｜파주 강동기 기자] 파주시는 아프리카돼지열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다.15일 파주시는 대규모 점포인 ㈜신세계사이먼과 협약 체결을 통해 소상공인들을 위한 1억 원의 특례보증 출연금을 이끌어내어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다. 협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소상공인들이 1억 원의 10배수인 10억 원(50여 명)의 대출\n",
      "text\n",
      "[일요서울｜파주 강동기 기자] 파주시는 아프리카돼지열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다.15일 파주시는 대규모 점포인 ㈜신세계사이먼과 협약 체결을 통해 소상공인들을 위한 1억 원의 특례보증 출연금을 이끌어내어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다. 협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소상공인들이 1억 원의 10배수인 10억 원(50여 명)의 대출\n",
      "sentences\n",
      "['[ 일요 서울｜ 파주 강동기 기자] 파주시는 아프리카 돼지 열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소 상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다.15', '일 파주시는 대규모 점포인 ㈜ 신세계 사이먼과 협약 체결을 통해 소 상공인들을 위한 1억 원의 특례보증 출연금을 이끌어 내 어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다.', '협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소 상공인들이 1억 원의 10 배 수인 10억 원 (50 여 명) 의 대출']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-153ba2b29a05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextRank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-9b2527da5be8>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGraphMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_sent_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_words_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-2d7479d77d1d>\u001b[0m in \u001b[0;36mbuild_sent_graph\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_sent_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mtfidf_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_mat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\venv\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1650\u001b[0m         \"\"\"\n\u001b[0;32m   1651\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1652\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\venv\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1058\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\venv\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    990\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "print(json_body)\n",
    "tr = TextRank(json_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
