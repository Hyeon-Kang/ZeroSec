{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Hyeon-Kang/ZeroSec/blob/master/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파주시, 아프리카돼지열병으로 인해 지역경제 유례없는 침체기\n",
      "[일요서울｜파주 강동기 기자] 파주시는 아프리카돼지열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다.15일 파주시는 대규모 점포인 ㈜신세계사이먼과 협약 체결을 통해 소상공인들을 위한 1억 원의 특례보증 출연금을 이끌어내어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다. 협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소상공인들이 1억 원의 10배수인 10억 원(50여 명)의 대출\n",
      "2019-11-15T14:40:55+09:00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# with를 이용해 파일을 연다.\n",
    "# json 파일은 같은 폴더에 있다고 가정!\n",
    "\n",
    "with open('news_sample.json', encoding='UTF-8') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "    # 문자열\n",
    "    # key가 json_string인 문자열 가져오기\n",
    "    json_title = json_data[\"title\"]\n",
    "    print(json_title)\n",
    "    \n",
    "    # 문자열\n",
    "    # key가 json_string인 문자열 가져오기\n",
    "    json_body = json_data[\"body\"]\n",
    "    print(json_body)\n",
    "\n",
    "    # 문자열\n",
    "    # key가 json_string인 문자열 가져오기\n",
    "    json_date = json_data[\"date\"]\n",
    "    print(json_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0XQZfMqxy47P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파주시, 아프리카돼지열병으로 인해 지역경제 유례없는 침체기\n"
     ]
    }
   ],
   "source": [
    "# 읽어온 데이터 확인\n",
    "print(json_title)\n",
    "sentence = []\n",
    "sentence.append(json_body.strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 단계\n",
    "# 소문자 변환\n",
    "# re 라이브러리로 특수문자 제거\n",
    "\n",
    "import re\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    sentence[i] = sentence[i].lower() # lower() : 모든 문자를 소문자로 변환\n",
    "    sentence[i] = re.sub(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\", ' ', sentence[i]) # 특수문자 제거\n",
    "    sentence[i] = re.sub(\"[.;:!\\'?,\\\"()|[\\]]\", ' ', sentence[i]) # 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 일요서울｜파주 강동기 기자  파주시는 아프리카돼지열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다 15일 파주시는 대규모 점포인 ㈜신세계사이먼과 협약 체결을 통해 소상공인들을 위한 1억 원의 특례보증 출연금을 이끌어내어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다  협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소상공인들이 1억 원의 10배수인 10억 원 50여 명 의 대출']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\venv\\tensorflow\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_token\n",
      "['파주시', ',', '아프리카', '돼지', '열병', '으로', '인해', '지역', '경제', '유례', '없는', '침체', '기']\n",
      "okt.morphs(sentence[0])\n",
      "['일요', '서울', '｜', '파주', '강', '동기', '기자', '파주시', '는', '아프리카', '돼지', '열병', '으로', '인해', '다수', '의', '가을', '축제', '와', '행사', '가', '취소', '돼', '지역', '경제', '가', '유례', '없는', '침체', '기를', '겪는', '상황', '에서', '소상', '공인', '들', '의', '경영', '난', '극복', '을', '위해', '다양한', '지', '원', '책', '을', '이끌어', '내고', '있다', '15일', '파주시', '는', '대규모', '점포', '인', '㈜', '신세계', '사이먼', '과', '협약', '체결', '을', '통해', '소상', '공인', '들', '을', '위', '한', '1억', '원', '의', '특례', '보증', '출연', '금', '을', '이끌어내어', '대규모', '점포', '와', '지역', '내', '중소', '상인', '과의', '상생', '발전', '을', '위', '한', '토대', '를', '마련', '했다', '협약', '과', '동시', '에', '담보', '능력', '이', '없어', '창업', '에', '어려움', '을', '겪거나', '경', '영', '난', '을', '겪고', '있는', '소', '상공', '인들', '이', '1억', '원', '의', '10', '배수', '인', '10억', '원', '50', '여', '명', '의', '대출']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "title_token = okt.morphs(json_title)\n",
    "print(\"title_token\")\n",
    "print(title_token)\n",
    "\n",
    "print(\"okt.morphs(sentence[0])\")\n",
    "print(okt.morphs(sentence[0]))\n",
    "\n",
    "test = okt.morphs(sentence[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 처리 - stop_word 리스트 불러오기 & 전처리\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "f = open(\"C:\\Python\\source\\stop_word.txt\", 'r', encoding='UTF-8')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    #print(line) #정상 출력 확인\n",
    "\n",
    "    stop_words.append(line.rstrip('\\n')) # 개행문자 제거 후 추가\n",
    "f.close()\n",
    "\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 저장 예시\n",
    "#stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
    "#stop_words=stop_words.split(' ')\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 일요서울｜파주 강동기 기자  파주시는 아프리카돼지열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다 15일 파주시는 대규모 점포인 ㈜신세계사이먼과 협약 체결을 통해 소상공인들을 위한 1억 원의 특례보증 출연금을 이끌어내어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다  협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소상공인들이 1억 원의 10배수인 10억 원 50여 명 의 대출']\n",
      "[' 일요서울｜파주 강동기 기자  파주시는 아프리카돼지열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다 15일 파주시는 대규모 점포인 ㈜신세계사이먼과 협약 체결을 통해 소상공인들을 위한 1억 원의 특례보증 출연금을 이끌어내어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다  협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소상공인들이 1억 원의 10배수인 10억 원 50여 명 의 대출']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 처리 테스트\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "result = [] \n",
    "for w in sentence: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "# 위의 4줄은 아래의 한 줄로 대체 가능\n",
    "# result=[word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "# 불용어 처리 결과 출력\n",
    "print(sentence) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'Punctuation'), ('일요', 'Noun'), ('서울', 'Noun'), ('｜', 'Foreign'), ('파주', 'Noun'), ('강', 'Noun'), ('동기', 'Noun'), ('기자', 'Noun'), (']', 'Punctuation'), ('파주시', 'Noun'), ('는', 'Josa'), ('아프리카', 'Noun'), ('돼지', 'Noun'), ('열병', 'Noun'), ('으로', 'Josa'), ('인해', 'Adjective'), ('다수', 'Noun'), ('의', 'Josa'), ('가을', 'Noun'), ('축제', 'Noun'), ('와', 'Josa'), ('행사', 'Noun'), ('가', 'Josa'), ('취소', 'Noun'), ('돼', 'Verb'), ('지역', 'Noun'), ('경제', 'Noun'), ('가', 'Josa'), ('유례', 'Noun'), ('없는', 'Adjective'), ('침체', 'Noun'), ('기를', 'Verb'), ('겪는', 'Verb'), ('상황', 'Noun'), ('에서', 'Josa'), ('소상', 'Noun'), ('공인', 'Noun'), ('들', 'Suffix'), ('의', 'Josa'), ('경영', 'Noun'), ('난', 'Josa'), ('극복', 'Noun'), ('을', 'Josa'), ('위해', 'Noun'), ('다양한', 'Adjective'), ('지', 'Modifier'), ('원', 'Modifier'), ('책', 'Noun'), ('을', 'Josa'), ('이끌어', 'Verb'), ('내고', 'Verb'), ('있다', 'Adjective'), ('.', 'Punctuation'), ('15일', 'Number'), ('파주시', 'Noun'), ('는', 'Josa'), ('대규모', 'Noun'), ('점포', 'Noun'), ('인', 'Josa'), ('㈜', 'Foreign'), ('신세계', 'Noun'), ('사이먼', 'Noun'), ('과', 'Josa'), ('협약', 'Noun'), ('체결', 'Noun'), ('을', 'Josa'), ('통해', 'Noun'), ('소상', 'Noun'), ('공인', 'Noun'), ('들', 'Suffix'), ('을', 'Josa'), ('위', 'Noun'), ('한', 'Josa'), ('1억', 'Number'), ('원', 'Noun'), ('의', 'Josa'), ('특례', 'Noun'), ('보증', 'Noun'), ('출연', 'Noun'), ('금', 'Noun'), ('을', 'Josa'), ('이끌어내어', 'Verb'), ('대규모', 'Noun'), ('점포', 'Noun'), ('와', 'Josa'), ('지역', 'Noun'), ('내', 'Noun'), ('중소', 'Noun'), ('상인', 'Noun'), ('과의', 'Josa'), ('상생', 'Noun'), ('발전', 'Noun'), ('을', 'Josa'), ('위', 'Noun'), ('한', 'Josa'), ('토대', 'Noun'), ('를', 'Josa'), ('마련', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation'), ('협약', 'Noun'), ('과', 'Josa'), ('동시', 'Noun'), ('에', 'Josa'), ('담보', 'Noun'), ('능력', 'Noun'), ('이', 'Josa'), ('없어', 'Adjective'), ('창업', 'Noun'), ('에', 'Josa'), ('어려움', 'Noun'), ('을', 'Josa'), ('겪거나', 'Verb'), ('경', 'Modifier'), ('영', 'Modifier'), ('난', 'Noun'), ('을', 'Josa'), ('겪고', 'Verb'), ('있는', 'Adjective'), ('소', 'Modifier'), ('상공', 'Noun'), ('인들', 'Josa'), ('이', 'Determiner'), ('1억', 'Number'), ('원', 'Noun'), ('의', 'Josa'), ('10', 'Number'), ('배수', 'Noun'), ('인', 'Josa'), ('10억', 'Number'), ('원', 'Noun'), ('(', 'Punctuation'), ('50', 'Number'), ('여', 'Noun'), ('명', 'Noun'), (')', 'Punctuation'), ('의', 'Noun'), ('대출', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "title_pos = okt.pos(json_body)\n",
    "print(title_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qK4W2YvHyruY"
   },
   "outputs": [],
   "source": [
    "# 미리 학습된 word2vec 모델 불러오기\n",
    "import gensim\n",
    "model = gensim.models.Word2Vec.load('C:\\Python\\source\\ko\\ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 유사도 계산 테스트\n",
    "#result=model.wv.most_similar(\"열병\")\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['일요', '서울', '파주', '강', '동기', '기자', '파주시', '아프리카', '돼지', '열병', '다수', '가을', '축제', '행사', '취소', '지역', '경제', '유례', '침체', '상황', '소상', '공인', '경영', '극복', '위해', '책', '파주시', '대규모', '점포', '신세계', '사이먼', '협약', '체결', '통해', '소상', '공인', '위', '원', '특례', '보증', '출연', '금', '대규모', '점포', '지역', '내', '중소', '상인', '상생', '발전', '위', '토대', '마련', '협약', '동시', '담보', '능력', '창업', '어려움', '난', '상공', '원', '배수', '원', '여', '명', '의', '대출']\n"
     ]
    }
   ],
   "source": [
    "#!pip install newspaper3k \n",
    "#!pip install jpype1\n",
    "#!pip install scikit-learn\n",
    "#from konlpy.tag import Kkma\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "noun = []\n",
    "# 문자열에서 명사만 추출\n",
    "noun = okt.nouns(json_body)\n",
    "print(noun)\n",
    "\n",
    "\n",
    "# TF-IDF Graph Matrix 생성 (주요 단어의 빈도수 비교)\n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "        \n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    \n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n",
    "    \n",
    "# Text Rank 알고리즘\n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "            \n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "    \n",
    "\n",
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        \n",
    "        if text[:5] in ('http:', 'https'):\n",
    "            self.sentences = self.sent_tokenize.url2sentences(text)\n",
    "        else:\n",
    "            self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        \n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "                    \n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        \n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        \n",
    "        self.word_rank_idx =  self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "        \n",
    "        \n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        \n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    def keywords(self, word_num=10):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        \n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "            \n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        \n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-67cc85ece7dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TextRank 결과 확인\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtextrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextRank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtextrank\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-bce32f2193fd>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mTextRank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentenceTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'http:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'https'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SentenceTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# TextRank 결과 확인\n",
    "textrank = TextRank(noun)\n",
    "for row in textrank.summarize(3):\n",
    "    print(row)\n",
    "    print()\n",
    "    print('keywords :',textrank.keywords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Komoron'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-784dcf68d94c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Komoron\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKomoron\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mkomo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKomoron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mk_title_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKomo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"komoron_title_token\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Komoron'"
     ]
    }
   ],
   "source": [
    "# Komoron\n",
    "from konlpy.tag import Komoron\n",
    "komo = Komoron()\n",
    "k_title_token = Komo.morphs(json_title)\n",
    "print(\"komoron_title_token\")\n",
    "print(k_title_token)\n",
    "\n",
    "print(\"komoron.morphs(sentence[0])\")\n",
    "print(komo.morphs(sentence[0]))\n",
    "\n",
    "k_test = komo.morphs(sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ploat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-67bd7b1fea27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# 차트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mploat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'ploat'"
     ]
    }
   ],
   "source": [
    "# 텍스트 유사도 비교 (TF-IDF, cosine inner projector 비교)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#test = [\"일요서울｜파주 강동기 기자] 파주시는 아프리카돼지열병으로 인해 다수의 가을 축제와 행사가 취소돼 지역경제가 유례없는 침체기를 겪는 상황에서 소상공인들의 경영난 극복을 위해 다양한 지원책을 이끌어 내고 있다.15일 파주시는 대규모 점포인 ㈜신세계사이먼과 협약 체결을 통해 소상공인들을 위한 1억 원의 특례보증 출연금을 이끌어내어 대규모 점포와 지역 내 중소 상인과의 상생발전을 위한 토대를 마련했다. 협약과 동시에 담보능력이 없어 창업에 어려움을 겪거나 경영난을 겪고 있는 소상공인들이 1억 원의 10배수인 10억 원(50여 명)의 대출\"]\n",
    "\n",
    "# 객체 생성\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 문장 벡터화 진행\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(test)\n",
    "\n",
    "# 각 단어\n",
    "text = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# 각 단어의 벡터 값\n",
    "idf = tfidf_vectorizer.idf_\n",
    "\n",
    "# 차트\n",
    "text.ploat(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사이킷런, 코사인 유사도\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
